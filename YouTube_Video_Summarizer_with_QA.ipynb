{"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["# YouTube Video Transcript Summarizer with Q&A"], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": ["This project allows users to enter a search topic or a YouTube video link to automatically fetch the transcript, generate a summary, and ask questions about the video content. If the transcript lacks the necessary information, the system performs a web search to provide a more accurate answer."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install -q requests youtube-transcript-api langchain langchain_community chromadb langchain_groq pytube tavily-python langchain-tavily"]}, {"cell_type": "code", "source": ["import os\n", "from google.colab import userdata\n", "\n", "YOUTUBE_API_KEY = userdata.get(\"YouTube\")\n", "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"Groq\")\n", "os.environ['TAVILY_API_KEY'] = userdata.get(\"Tavily\")"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["By using the YouTube Data API v3 to search videos and extract the first video ID, users can enter general topics instead of specific video URL."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["import requests\n", "\n", "def search_youtube_video(query):\n", "  url = \"https://www.googleapis.com/youtube/v3/search\"\n", "  params = {\n", "      \"part\": \"snippet\",\n", "      \"q\": query,\n", "      \"type\": \"video\",\n", "      \"maxResults\": 1,\n", "      \"key\": YOUTUBE_API_KEY\n", "  }\n", "\n", "  response = requests.get(url, params=params)\n", "  results = response.json()\n", "\n", "  if results.get(\"items\"):\n", "    video = results[\"items\"][0]\n", "    video_id = video[\"id\"][\"videoId\"]\n", "    title = video[\"snippet\"][\"title\"]\n", "    print(f\"Found YouTube video: {title} (https://www.youtube.com/watch?v={video_id})\")\n", "    return video_id\n", "\n", "  return None"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Using the video ID, this function attempts to fetch the English transcript through the YouTubeTranscriptApi."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from youtube_transcript_api import YouTubeTranscriptApi\n", "\n", "def fetch_transcript(video_id):\n", "  try:\n", "    transcript = YouTubeTranscriptApi().fetch(video_id, languages=['en', 'en-US', 'en-GB'])\n", "    text = \" \".join([entry.text for entry in transcript])\n", "    return text\n", "  except Exception as e:\n", "    return None"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Because transcripts can be very long, this function splits the text into overlapping chunks. This chunking helps ensure that the vector search engine can effectively find relevant pieces of text when the user asks questions, without losing important context."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "\n", "def split_into_chunks(text):\n", "  splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n", "  documents = splitter.create_documents([text])\n", "  return documents"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["This function converts the text chunks into vector embeddings using a pre-trained SentenceTransformer model. These embeddings are stored in Chroma database, which enables fast similarity search."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from langchain.embeddings import HuggingFaceEmbeddings\n", "from langchain.vectorstores import Chroma\n", "\n", "def store_in_db(documents):\n", "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n", "  vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory=\"chroma_db\")\n", "  return vectorstore"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Language Model Setup and Prompt Templates: sets up the LLM, prompt templates, and chains for routing user questions and performing retrieval-augmented generation (RAG)."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from langchain_groq import ChatGroq\n", "from langchain.prompts import PromptTemplate\n", "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n", "from langchain import hub\n", "\n", "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)\n", "\n", "router_system_msg = \"\"\"\n", "You are given a user's question and some context from a video transcript.\n", "Decide whether the question can be answered using only the given context.\n", "\n", "If yes, return \"Can respond\".\n", "If not, return \"Search required\".\n", "\n", "Return a JSON with a single key 'decision'. No explanation or extra text.\n", "\"\"\"\n", "\n", "router_user_msg = \"Context:\\n{context}\\n\\nQuestion: {question}\"\n", "\n", "router_prompt = PromptTemplate(\n", "    template=router_system_msg + \"\\n\\n\" + router_user_msg,\n", "    input_variables=[\"question\", \"context\"]\n", ")\n", "\n", "router_chain = router_prompt | llm | JsonOutputParser()\n", "\n", "rag_prompt = hub.pull(\"feefe/rag_llama3_1\")\n", "rag_chain = rag_prompt | llm | StrOutputParser()"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["If the router decides the transcript doesn\u2019t have enough information, this function performs a web search using the TavilySearch tool. It collects the top results content into a single document, which is then used as external context for the RAG model to generate a relevant answer."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from langchain_tavily import TavilySearch\n", "from langchain.schema import Document\n", "\n", "def do_web_search(query: str, max_res=3) -> Document:\n", "  web_search_tool = TavilySearch(max_results=max_res, search_depth='advanced')\n", "  results = web_search_tool.invoke({\"query\": query})\n", "  results_content = \"\\n\".join([res[\"content\"] for res in results['results']])\n", "  return Document(page_content=results_content)"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["This is the core function for processing and generating answers. It retrieves context from transcript vectorstore and uses the router to determine if it\u2019s sufficient. If not, performs a web search and answers using a RAG prompt."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["def search_and_answer(qa_chain, query):\n", "  retrieved_docs = qa_chain.retriever.get_relevant_documents(query)\n", "  transcript_context = \"\\n\".join(doc.page_content for doc in retrieved_docs[:3])\n", "\n", "  decision_response = router_chain.invoke({\n", "      \"question\": query,\n", "      \"context\": transcript_context\n", "  })\n", "  decision = decision_response.get(\"decision\", \"\").strip().lower()\n", "\n", "  if decision == \"can respond\":\n", "    response = qa_chain.invoke(query)\n", "    result = response.get(\"result\", \"\") if isinstance(response, dict) else response\n", "\n", "    no_answer_phrases = [\"i don't\", \"not sure\", \"cannot answer\", \"no information\", \"not mentioned\", \"not related\"]\n", "\n", "    if result.strip() and all(phrase not in result.lower() for phrase in no_answer_phrases):\n", "      return result\n", "\n", "  print(\"No relevant information in transcript. Switching to web search...\")\n", "  web_doc = do_web_search(query)\n", "  rag_response = rag_chain.invoke({\n", "      \"question\": query,\n", "      \"context\": web_doc.page_content\n", "  })\n", "\n", "  return rag_response"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["RetrievalQA chain for question answering is built to generate factual responses. For this task, its temperature is set to zero, thus decreasing the chance for hallucinations."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from langchain.chains import RetrievalQA\n", "\n", "def build_factual_chain(vectorstore):\n", "  factual_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.0)\n", "  factual_chain = RetrievalQA.from_chain_type(llm=factual_llm, retriever=vectorstore.as_retriever())\n", "\n", "  return factual_chain"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["A separate prompt-based summary chain has been created for summarization, using a temperature of 0.4 to allow some creativity while keeping the output controlled. To handle large transcripts that exceed the model's context limits, each chunk is summarized individually. These partial summaries are then combined and summarized again to produce a final summary."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["summary_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.4)\n", "\n", "summary_prompt = PromptTemplate.from_template(\"\"\"\n", "You are a helpful assistant. Analyze the following video transcript, extract the key points and provide a structured, concise summary.\n", "\n", "Transcript:\n", "{transcript}\n", "\n", "Return a concise paragraph summarizing the video without any introductory phrases.\n", "\"\"\")\n", "\n", "summary_chain = summary_prompt | summary_llm | StrOutputParser()\n", "\n", "def summarize_transcript(docs):\n", "  summaries = []\n", "  for doc in docs:\n", "      partial = summary_chain.invoke({\"transcript\": doc.page_content})\n", "      summaries.append(partial)\n", "\n", "  summary = summary_chain.invoke({\"transcript\": \"\\n\".join(summaries)})\n", "  return summary"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Simple utility function that wraps the text so that it is easier to read the model's output:"], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["def wrap_and_print(text, width=150):\n", "  import textwrap\n", "  wrapped_text = textwrap.fill(text, width=width)\n", "  print(wrapped_text)"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["This section handles user input of a topic or YouTube video link and supports interactive Q&A until the user exits."], "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["from pytube import extract\n", "\n", "user_input = input(\"Enter a topic or YouTube video link: \").strip()\n", "\n", "if \"www.youtube.com\" in user_input:\n", "  video_id = extract.video_id(user_input)\n", "else:\n", "  video_id = search_youtube_video(user_input)\n", "\n", "if not video_id:\n", "  print(\"No video found.\")\n", "  exit()\n", "\n", "transcript = fetch_transcript(video_id)\n", "if not transcript:\n", "  print(\"Couldn't fetch transcript.\")\n", "  exit()\n", "\n", "docs = split_into_chunks(transcript)\n", "\n", "summary = summarize_transcript(docs)\n", "print(\"\\nVideo Summary:\")\n", "wrap_and_print(summary)\n", "\n", "vectordb = store_in_db(docs)\n", "factual_chain = build_factual_chain(vectordb)\n", "\n", "while True:\n", "  question = input(\"\\nAsk a question (or type 'exit'): \").strip()\n", "  if question.lower() in (\"exit\"):\n", "    break\n", "\n", "  answer = search_and_answer(factual_chain, question)\n", "\n", "  print(\"\\nAnswer:\")\n", "  wrap_and_print(answer)"], "metadata": {}, "execution_count": null, "outputs": []}]}